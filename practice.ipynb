{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsd09\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2304])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsd09\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2',output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a587726fe0a44dfa8af95694f43c8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e27470cf34421ea8ece42511593afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b6731621834b27891330125f69fa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d72fd649114adc95c043ada8f9edba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 2 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 3 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 4 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 5 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 6 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 7 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 8 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 9 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 10 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 11 attention shape: torch.Size([1, 12, 6, 6])\n",
      "Layer 12 attention shape: torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n",
    "\n",
    "# GPT-2 모델을 output_attentions=True로 설정하여 로드\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 입력 텍스트\n",
    "input_ids = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# 모델을 통해 입력을 전달하여 출력과 어텐션 가중치 받기\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# 출력 중 어텐션 가중치를 추출\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# 각 레이어의 어텐션 가중치 출력\n",
    "for layer, attention in enumerate(attentions):\n",
    "    print(f\"Layer {layer + 1} attention shape: {attention.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winogender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n",
      "Keyword arguments {'add_space_before_punct_symbol': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_string:tensor([  464, 33024,  1297,   262,  6491,   326,   673])\n",
      "alt_strings:tensor([  464, 33024,  1297,   262,  6491,   326,   339])\n"
     ]
    }
   ],
   "source": [
    "examples = winogender.load_examples()\n",
    "interventions = [ex.to_intervention(tokenizer, 'bls') for ex in examples]\n",
    "x = interventions[0].base_strings_tok[0]\n",
    "x_alt = interventions[0].base_strings_tok[1]\n",
    "print(f\"base_string:{x}\")\n",
    "print(f\"alt_strings:{x_alt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġtechnician', 'Ġtold', 'Ġthe', 'Ġcustomer', 'Ġthat', 'Ġshe']\n",
      "['The', 'Ġtechnician', 'Ġtold', 'Ġthe', 'Ġcustomer', 'Ġthat', 'Ġhe']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(x))\n",
    "print(tokenizer.convert_ids_to_tokens(x_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x_alt).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next token:  had\n"
     ]
    }
   ],
   "source": [
    "# 마지막 토큰에 대한 로짓을 선택\n",
    "last_token_logits = logits[-1,:]\n",
    "\n",
    "# 소프트맥스 함수로 확률 계산\n",
    "probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "# 가장 높은 확률을 가진 토큰 선택\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Predicted next token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -41.3759, -126.2039,  -89.6250, -102.7298,  -97.1775, -102.6191,\n",
       "        -136.7418], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model(x)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' could pay with cash.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(interventions[0].candidates_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' had completed the repair.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(interventions[0].candidates_tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 33024, 1297, 262, 6491, 326, 673]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prime = x.tolist()\n",
    "batch = x_prime + interventions[0].candidates_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464, 33024,  1297,   262,  6491,   326,   673,   714,  1414,   351,\n",
       "          5003]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_prime = torch.tensor(batch[:-1]).unsqueeze(dim=0)\n",
    "batch_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 50257])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(batch_prime)[0]\n",
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50257])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_logits = F.log_softmax(logits[-1,:,:],dim=-1)\n",
    "log_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(x.clone().detach().unsqueeze(0))\n",
    "attn = output.attentions\n",
    "len(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[0][0][1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464, 33024,  1297,   262,  6491,   326,   673]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.clone().detach().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = torch.ones([1,3],dtype=bool)\n",
    "A = torch.zeros([1,3])\n",
    "A = torch.where(condition,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2',output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embed = model.transformer.wte(x)\n",
    "model.transformer.h[0].attn.c_attn(token_embed).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = model.transformer.h[0].attn.c_attn(token_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2304])\n",
      "torch.Size([3, 2304])\n",
      "torch.Size([1, 2304])\n"
     ]
    }
   ],
   "source": [
    "q,k,v = tmp_x.split(3,dim=0)\n",
    "print(q.size())\n",
    "print(k.size())\n",
    "print(v.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2304])\n",
      "torch.Size([3, 2304])\n",
      "torch.Size([1, 2304])\n"
     ]
    }
   ],
   "source": [
    "q,k,v = tmp_x.chunk(3,dim=0)\n",
    "print(q.size())\n",
    "print(k.size())\n",
    "print(v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = lambda layer: model.transformer.h[layer].attn\n",
    "attention_layer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bias',\n",
       "              tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "                        [ True,  True, False,  ..., False, False, False],\n",
       "                        [ True,  True,  True,  ..., False, False, False],\n",
       "                        ...,\n",
       "                        [ True,  True,  True,  ...,  True, False, False],\n",
       "                        [ True,  True,  True,  ...,  True,  True, False],\n",
       "                        [ True,  True,  True,  ...,  True,  True,  True]]]])),\n",
       "             ('masked_bias', tensor(-10000.))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer(1)._buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 12, 192])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_shape = tmp_x.size()[:-1] + (12,tmp_x.size(-1)//attention_layer(1).num_heads)\n",
    "tmp_x.view(*new_shape).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2304])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2304])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('param', tensor([[0.6614, 0.2669],\n",
      "        [0.0617, 0.6213]])), ('buff', tensor([[-0.4519, -0.1661],\n",
      "        [-1.5228,  0.3817]]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.param = nn.Parameter(torch.randn([2, 2]))\n",
    "\t\t\n",
    "\t\tbuff = torch.randn([2, 2])\n",
    "\t\tself.register_buffer('buff', buff)\n",
    "\n",
    "\t\tself.non_buff = torch.randn([2, 2])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x\n",
    "\n",
    "model = Model()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
